---
title: "Verification of Sample Size Calculation via Simulation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Verification of Sample Size Calculation via Simulation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(gsDesignNB)
library(data.table)
library(ggplot2)
```

## Introduction

This vignette verifies the accuracy of the `sample_size_nbinom` function by comparing its theoretical predictions for average exposure, statistical information, and power against results from a large-scale simulation.

We specifically test a scenario with:
*   Piecewise constant accrual rates.
*   Piecewise exponential dropout (constant in this example).
*   Negative binomial outcomes.
*   Fixed follow-up design.

## Simulation Design

The simulation parameters are chosen to yield a sample size of approximately 200 subjects.

### Parameters

*   **Rates**: $\lambda_1 = 0.4$ (Control), $\lambda_2 = 0.3$ (Experimental).
*   **Dispersion**: $k = 0.5$.
*   **Power**: 90%.
*   **Alpha**: 0.025 (one-sided).
*   **Dropout**: 10% per unit time ($\delta = 0.1$).
*   **Trial Duration**: 24 months.
*   **Max Follow-up**: 12 months.
*   **Event Gap**: 30 days (approx 0.082 years).
*   **Accrual**: Piecewise linear ramp-up over 12 months (Rate $R$ for 0-6mo, $2R$ for 6-12mo).

### Theoretical Calculation

First, we calculate the required sample size and expected properties using `sample_size_nbinom`.

```{r design}
# Parameters
lambda1 <- 0.4
lambda2 <- 0.3
dispersion <- 0.5
power <- 0.9
alpha <- 0.025
dropout_rate <- 0.1
max_followup <- 12
trial_duration <- 24
event_gap <- 30 / 365.25

# Accrual targeting N ~ 200
# (Pre-calculated rates)
accrual_rate <- c(11, 22)
accrual_duration <- c(6, 6)

design <- sample_size_nbinom(
  lambda1 = lambda1, lambda2 = lambda2, dispersion = dispersion,
  power = NULL, # Calculate power for this specific design
  alpha = alpha, sided = 1,
  accrual_rate = accrual_rate,
  accrual_duration = accrual_duration,
  trial_duration = trial_duration,
  dropout_rate = dropout_rate,
  max_followup = max_followup,
  event_gap = event_gap,
  method = "friede"
)

print(design)
```

## Simulation Results

We simulated 10,000 trials using the parameters defined above. The simulation script is located in `data-raw/generate_simulation_data.R`.

```{r load_results}
# Load pre-computed simulation results
results_file <- system.file("extdata", "simulation_results.rds", package = "gsDesignNB")

if (results_file == "" && file.exists("../inst/extdata/simulation_results.rds")) {
  results_file <- "../inst/extdata/simulation_results.rds"
}

if (results_file != "") {
  sim_data <- readRDS(results_file)
  results <- sim_data$results
  design_ref <- sim_data$design
} else {
  # Fallback if data is not available (e.g. not installed yet)
  # This block allows the vignette to build without the data, but warns.
  warning("Simulation results not found. Skipping verification plots.")
  results <- NULL
  design_ref <- design
}
```

### 1. Average Exposure Verification

We compare the theoretical average exposure calculated by `sample_size_nbinom` with the observed average exposure in the simulation.

```{r exposure, eval=!is.null(results)}
# Theoretical Exposure
theo_exposure <- design_ref$exposure

# Observed Exposure (Average across all trials and arms)
# Note: Exposure is the same for both arms in this design (randomized)
obs_exposure <- mean(c(results$exposure_control, results$exposure_experimental))

comparison_exp <- data.frame(
  Metric = "Average Exposure",
  Theoretical = theo_exposure,
  Simulated = obs_exposure,
  Difference = obs_exposure - theo_exposure,
  Rel_Diff_Pct = 100 * (obs_exposure - theo_exposure) / theo_exposure
)

knitr::kable(comparison_exp, digits = 4, caption = "Comparison of Average Exposure")
```

The theoretical exposure should closely match the simulated average exposure.

### 2. Statistical Information and Variance

The theoretical variance of the log rate ratio estimator (Wald test) is given by:

$$ V_{theo} = \frac{1/\mu_1 + k_{adj}}{n_1} + \frac{1/\mu_2 + k_{adj}}{n_2} $$

where $\mu_i = \lambda_{i,eff} \bar{t}$ is the expected number of events per subject in group $i$ (using effective rates adjusted for event gaps), and $k_{adj} = k \cdot Q$ is the dispersion parameter inflated for variable follow-up.

The dispersion parameter $k$ directly increases the variance of the estimator. In a standard Poisson model, $k=0$, and the variance depends only on the expected number of events. With negative binomial dispersion ($k > 0$), the variance is inflated, reflecting the extra variability (overdispersion) in the data.

We compare this with the variance of the estimates from the simulation.

```{r variance, eval=!is.null(results)}
# Theoretical Variance (from design object)
theo_var <- design_ref$variance

# Empirical Variance of the Log Hazard Ratio estimates
emp_var <- var(results$estimate, na.rm = TRUE)

# Average Squared Standard Error (mean of the estimated variances)
avg_se_sq <- mean(results$se^2, na.rm = TRUE)

comparison_var <- data.frame(
  Metric = c("Variance of Estimator"),
  Theoretical = theo_var,
  Empirical_Var = emp_var,
  Avg_Estimated_Var = avg_se_sq
)

knitr::kable(comparison_var, digits = 5, caption = "Comparison of Variance")
```

*   **Empirical Var**: The actual variability of the estimated log rate ratios across 10,000 trials.
*   **Avg Estimated Var**: The average of the variance estimates ($SE^2$) produced by the Wald test in each trial.

Close agreement indicates that the sample size formula correctly anticipates the variability of the test statistic.

### 3. Power Verification

Finally, we compare the theoretical power with the empirical power (proportion of trials rejecting the null hypothesis).

```{r power, eval=!is.null(results)}
# Theoretical Power
theo_power <- design_ref$power

# Empirical Power
emp_power <- mean(results$p_value < design_ref$inputs$alpha, na.rm = TRUE)

comparison_pwr <- data.frame(
  Metric = "Power",
  Theoretical = theo_power,
  Simulated = emp_power,
  Difference = emp_power - theo_power
)

knitr::kable(comparison_pwr, digits = 4, caption = "Comparison of Power")
```

A 95% confidence interval for the empirical power can be calculated to check if the theoretical power falls within the simulation error bounds.

```{r power_ci, eval=!is.null(results)}
binom.test(sum(results$p_value < design_ref$inputs$alpha, na.rm = TRUE), nrow(results))$conf.int
```

## Conclusion

The simulation results confirm that `sample_size_nbinom` accurately predicts average exposure, variance, and power for this complex design with piecewise accrual and dropout.

